{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Define transformations","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.models import vgg16\nfrom torch.cuda.amp import autocast,GradScaler\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n# Define transformations\ndef get_data_loaders(batch_size=128):\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomCrop(32, padding=4),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return trainloader, testloader\n\n\n\n  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LOad and modify VGG16 model","metadata":{}},{"cell_type":"code","source":"def get_model():\n    vgg = torchvision.models.vgg16(pretrained=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  Modify the feature extractor for CIFAR-10 input","metadata":{}},{"cell_type":"code","source":"vgg.avgpool = nn.AdaptiveAvgPool2d((1, 1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Replace the classifier with a smaller one and reduced dropout","metadata":{}},{"cell_type":"code","source":"vgg.classifier = nn.Sequential(\n        nn.Linear(512, 256),\n        nn.ReLU(inplace=True),\n        nn.Dropout(0.3),  # Smaller dropout\n        nn.Linear(256, 10)  # 10 classes for CIFAR-10\n    )\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Unfreeze all layers","metadata":{}},{"cell_type":"code","source":"for param in vgg.parameters():\n        param.requires_grad = True\n\n    return vgg\n\ndef load_model(device=\"cuda\"):\n    \"\"\"Load modified VGG16 model for CIFAR-10 with adjusted classifier.\"\"\"\n    model = get_model()  # use the updated get_model\n    state_dict = torch.load(\"/kaggle/input/vgg16_cifar10_best1/pytorch/default/1/vgg16_cifar10_best.pth\", map_location=\"cpu\")\n    model.load_state_dict(state_dict)\n    model.to(device)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training function","metadata":{}},{"cell_type":"code","source":"def train_model(model, trainloader, criterion, optimizer, device, epochs=10):\n    model.to(device)\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in trainloader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluation function","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, testloader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in testloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n    return accuracy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save model function","metadata":{}},{"cell_type":"code","source":"def save_model(model, path=\"vgg16_cifar10.pth\"):\n    torch.save(model.state_dict(), path)\n    print(f\"Model saved to {path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Main script execution","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrainloader, testloader = get_data_loaders()\nmodel = get_model()\nmodel = load_model()\n\ncriterion = nn.CrossEntropyLoss()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n\nepoch_set = 200","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install thop","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Knowledge Ditillation","metadata":{}},{"cell_type":"code","source":"def knowledge_distillation(teacher, student, dataloader, device=\"cuda\"):\n    loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n    optimizer = optim.Adam(student.parameters(), lr=1e-4)\n    student.train()\n    for images, _ in dataloader:\n        images = images.to(device)\n        with torch.no_grad():\n            teacher_outputs = teacher(images)\n        student_outputs = student(images)\n        loss = loss_fn(student_outputs.log_softmax(dim=1), teacher_outputs.softmax(dim=1))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\ndef mixup_data(x, y, alpha=1.0):\n    lam = np.random.beta(alpha, alpha)\n    index = torch.randperm(x.size(0)).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Helper Function","metadata":{}},{"cell_type":"code","source":"def fine_tune_model(model, trainloader, testloader, epochs=epoch_set, lr=0.01, weight_decay=5e-4, accumulation_steps=4, device=\"cuda\"):\n    model.to(device)\n    model.train()\n\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n    scaler = GradScaler()\n\n    best_acc = 0\n    early_stop_counter = 0\n    patience = 12\n\nfor epoch in range(epochs):\n        running_loss = 0.0\n        correct, total = 0, 0\n        optimizer.zero_grad()\n\n        for i, (images, labels) in enumerate(trainloader):\n            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n\n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels) / accumulation_steps\n\n            scaler.scale(loss).backward()\n\nif (i + 1) % accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n            running_loss += loss.item() * accumulation_steps\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            # Free memory every 5 steps\n            if i % 5 == 0:\n                del images, labels, outputs, loss, predicted\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n\ntest_acc = test_model(model, testloader)\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n\n        scheduler.step()\n\n        if test_acc > best_acc:\n            best_acc = test_acc\n            early_stop_counter = 0\n\n        else:\n            early_stop_counter += 0\n            if early_stop_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    print(\"Fine-tuning complete. Best Test Accuracy:\", best_acc)\n    return best_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fine Tune Function","metadata":{}},{"cell_type":"code","source":"def fine_tune_base_model(model, trainloader, testloader, epochs=epoch_set, lr=0.01, weight_decay=5e-4, accumulation_steps=4, device=\"cuda\"):\n    model.to(device)\n    model.train()\n\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n    scaler = GradScaler()\n\n    best_acc = 0\n    early_stop_counter = 0\n    patience = 5 # Stop early if no improvement in 2 epochs\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        loop = tqdm(trainloader, desc=f\"Epoch [{epoch+1}/{epochs}]\")\n\n        for images, labels in loop:\n            images, labels = images.to(device), labels.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Apply Mixup","metadata":{}},{"cell_type":"code","source":"images, y_a, y_b, lam = mixup_data(images, labels, alpha=1.0)\n            outputs = model(images)\n            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            loop.set_postfix(loss=loss.item())\n\n        scheduler.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluate","metadata":{}},{"cell_type":"code","source":"model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in testloader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n        acc = 100. * correct / total\n        print(f\"Epoch {epoch+1} - Test Accuracy: {acc:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save best model","metadata":{}},{"cell_type":"code","source":"if acc > best_acc:\n            best_acc = acc\n            print(f\"Accuracy: {acc:.2f}%\")\n    return model, best_acc\n\nfrom torch.cuda.amp import autocast, GradScaler\nimport torch.nn.functional as F\n\ndef fine_tune_with_kd(student_model, teacher_model, trainloader, testloader,\n                     epochs=600, lr=0.0001, weight_decay=1e-4, \n                     alpha=0.7, temperature=4.0,  # Changed: alpha=0.7, temperature=4.0\n                     device=\"cuda\"):\n    student_model.to(device)\n    teacher_model.to(device)\n    teacher_model.eval()\n    student_model.train()\n    optimizer = optim.AdamW(student_model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    ce_criterion = nn.CrossEntropyLoss()\n    scaler = GradScaler()\n\n    best_acc = 0.0\n    early_stop_counter = 0\n\n    for epoch in range(epochs):\n        student_model.train()\n        running_loss = 0.0\n        correct, total = 0, 0\n        optimizer.zero_grad()\n\nfor i, (images, labels) in enumerate(trainloader):\n            images, labels = images.to(device), labels.to(device)\n            with autocast():\n                student_outputs = student_model(images)\n                with torch.no_grad():\n                    teacher_outputs = teacher_model(images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Updated KD loss with temperature scaling","metadata":{}},{"cell_type":"code","source":"kd_loss = F.kl_div(\n                    F.log_softmax(student_outputs / temperature, dim=1),\n                    F.softmax(teacher_outputs / temperature, dim=1),\n                    reduction=\"batchmean\"\n                ) * (temperature ** 2)\n                ce_loss = ce_criterion(student_outputs, labels)\n                loss = alpha * kd_loss + (1 - alpha) * ce_loss  # Weighted loss\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            _, predicted = student_outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n\nscheduler.step()\n        test_acc = test_model(student_model, testloader, device=device)\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(student_model.state_dict(), \"best_kd_student.pth\")\n            early_stop_counter = 0\n\n    print(f\"✅ KD fine-tuning complete. Best Test Accuracy: {best_acc:.2f}%\")\n    return best_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compute Model Accuracy","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom thop import profile\nimport copy\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.autograd import grad\nfrom collections import defaultdict\n\n\ndef compute_accuracy(model, dataloader, device=\"cuda\"):\n    model.to(device).eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Count Non-Zero Parameters","metadata":{}},{"cell_type":"code","source":"def count_non_zero_parameters(model):\n    return sum((param.ne(0)).sum().item() for param in model.parameters() if param.requires_grad)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Count Non-Zero Convolutional Filters","metadata":{}},{"cell_type":"code","source":"def count_nonzero_filters(model):\n    \"\"\"Count the total number of remaining convolutional filters (non-zero ones) in the model.\"\"\"\n    total_filters = 0\n    for layer in model.modules():\n        if isinstance(layer, nn.Conv2d):\n            # Count only filters (out_channels) that contain at least one non-zero weight\n            nonzero_filters = (layer.weight.abs().sum(dim=(1, 2, 3)) > 0).sum().item()\n            total_filters += nonzero_filters\n    return total_filters\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compute Non-Zero FLOPs ","metadata":{}},{"cell_type":"code","source":"def compute_nonzero_flops(model, input_size=(1, 3, 32, 32), device=\"cuda\"):\n    model.to(device)\n    inputs = torch.randn(input_size).to(device)\n    total_flops, _ = profile(model, inputs=(inputs,), verbose=False)\n    total_nonzero_flops = 0\n    for layer in model.modules():\n        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n            total_weights = layer.weight.numel()\n            nonzero_weights = (layer.weight.abs() > 0).sum().item()\n            sparsity_ratio = nonzero_weights / total_weights if total_weights > 0 else 0\n            total_nonzero_flops += total_flops * sparsity_ratio\n    return total_nonzero_flops / 1e6\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test the Model on a Test Set","metadata":{}},{"cell_type":"code","source":"def test_model(model, testloader, device=\"cuda\"):\n    model.eval()\n    correct, total = 0, 0\n    criterion = nn.CrossEntropyLoss()\n    loss_total = 0\n    with torch.no_grad():\n        for images, labels in testloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss_total += criterion(outputs, labels).item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    return (correct / total) * 100\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fusion Pruning Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n\ndef differential_sensitivity_fusion_pruning(model, dataloader, device=\"cuda\"):\n    \"\"\"\n    Prunes filters based on differential sensitivity fusion of three importance metrics\n    \"\"\"\n    model.to(device).eval()\n    importance = {}\n    activations, gradients = {}, {}\n\n    def forward_hook(name, module, input, output):\n        activations[name] = output.detach()\n\n    def backward_hook(name, module, grad_input, grad_output):\n        gradients[name] = grad_output[0].detach()\n\nhooks = []\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            hooks.append(module.register_forward_hook(partial(forward_hook, name)))\n            hooks.append(module.register_backward_hook(partial(backward_hook, name)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Forward and backward pass","metadata":{}},{"cell_type":"code","source":"images, labels = next(iter(dataloader))\n    images, labels = images.to(device), labels.to(device)\n    outputs = model(images)\n    loss = F.cross_entropy(outputs, labels)\n    loss.backward()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remove hooks","metadata":{}},{"cell_type":"code","source":"for hook in hooks:\n        hook.remove()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Calculate importance for each convolutional layer","metadata":{}},{"cell_type":"code","source":"for name in activations.keys():\n        if name not in gradients:\n            continue\n            \n        activation, grad = activations[name], gradients[name]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compute components","metadata":{}},{"cell_type":"code","source":"grad_sensitivity = grad.abs().mean(dim=(0, 2, 3))\n        weight = dict(model.named_parameters())[f\"{name}.weight\"]\n        taylor_expansion = (grad.mean(dim=(2, 3)) * weight.norm(p=2, dim=(1, 2, 3))).abs().mean(dim=0)\n        \n        batch_mean_activation = activation.mean(dim=(2, 3), keepdim=True)\n        activation_prob = F.softmax(activation, dim=1)\n        batch_mean_prob = F.softmax(batch_mean_activation, dim=1) + 1e-10\n        kl_divergence = F.kl_div(batch_mean_prob.log(), activation_prob, reduction=\"none\").sum(dim=(0, 2, 3))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Differential sensitivity fusion","metadata":{}},{"cell_type":"code","source":"diff1 = torch.exp(torch.abs(grad_sensitivity - taylor_expansion))\n        diff2 = torch.exp(torch.abs(taylor_expansion - kl_divergence))\n        diff3 = torch.exp(torch.abs(grad_sensitivity - kl_divergence))  \n        \n        sensitivity_fusion = diff1 + diff2 + 0.5 * diff3 \n\n        importance[name] = sensitivity_fusion.detach().cpu().numpy()\n    \n    return importance","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bandit Agent for pruning","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models\nimport numpy as np\nimport random\nimport gc\n\nclass Banditagent:\n    def __init__(self, num_layers, lr=0.01, gamma=0.99, buffer_size=512, mixing_net_hidden=32, epsilon_decay=0.995):\n        self.num_layers = num_layers\n        self.lr = lr\n        self.gamma = gamma\n        self.buffer_size = buffer_size\n        self.epsilon = 1.0  # Initial exploration rate\n        self.epsilon_decay = epsilon_decay\n        # Initialize Q-table with percentage values between 10-90%\n        self.q_table = {layer: random.uniform(10, 90) for layer in range(num_layers)}\n        self.replay_buffer = []\n        self.mixing_net = nn.Sequential(\n            nn.Linear(1, mixing_net_hidden),\n            nn.ReLU(),\n            nn.Linear(mixing_net_hidden, 1)\n        ).to(\"cuda\")\n        self.optimizer = optim.Adam(self.mixing_net.parameters(), lr=lr)\n\ndef select_action(self, layer_idx, base_pruning_rate):\n        \"\"\"\n        Select pruning rate for a specific layer based on MIX policy.\n        \n        Args:\n            layer_idx: Index of the layer\n            base_pruning_rate: Base pruning rate (in percentage, e.g., 50, 60, 70)\n            \n        Returns:\n            Pruning rate for the specified layer (in percentage)\n        \"\"\"\n        # Ensure base_pruning_rate is between 0 and 100\n        base_pruning_rate = min(100, max(0, base_pruning_rate))\n        \n        # Epsilon-greedy policy\n        if random.random() < self.epsilon:\n            # Use a range around the base pruning rate to explore (within ±20%)\n            min_rate = max(10, base_pruning_rate - 5)\n            max_rate = min(90, base_pruning_rate + 5)\n            return random.uniform(min_rate, max_rate)\n        \n        # Use learned Q-value for this layer\n        return self.q_table[layer_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Store","metadata":{}},{"cell_type":"code","source":"def store_experience(self, state, action, reward):\n        \"\"\"Store experience in replay buffer\"\"\"\n        if len(self.replay_buffer) >= self.buffer_size:\n            self.replay_buffer.pop(0)\n        self.replay_buffer.append((state, action, reward))\n\n    def update_q_values(self):\n        \"\"\"Update Q-values based on experiences in replay buffer\"\"\"\n        if len(self.replay_buffer) < 32:  # Minimum batch size\n            return \n            \nbatch = random.sample(self.replay_buffer, min(len(self.replay_buffer), 64))\n        states, actions, rewards = zip(*batch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Convert to tensors","metadata":{}},{"cell_type":"code","source":"states = torch.tensor(states, dtype=torch.float32).to(\"cuda\")\n        actions = torch.tensor(actions, dtype=torch.float32).view(-1, 1).to(\"cuda\")\n        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1).to(\"cuda\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compute target Q-values","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n            target_q_values = self.mixing_net(states) + rewards\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compute current Q-values","metadata":{}},{"cell_type":"code","source":"current_q_values = self.mixing_net(states)\n\nloss = nn.MSELoss()(current_q_values, target_q_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Update Perameters","metadata":{}},{"cell_type":"code","source":"self.optimizer.zero_grad()\n        loss.requires_grad = True\n        loss.backward()\n        self.optimizer.step()\n\n self.epsilon = max(self.epsilon * self.epsilon_decay, 0.01)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Update Q-table","metadata":{}},{"cell_type":"code","source":"for layer_idx in range(self.num_layers):\n            state_tensor = torch.tensor([[50]], dtype=torch.float32).to(\"cuda\")  # Use 50% as reference state\n            predicted_value = self.mixing_net(state_tensor).item() * 100  # Scale to percentage\n            # Ensure value is within reasonable range (10-90%)\n            self.q_table[layer_idx] = max(10, min(90, predicted_value))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Improve bandit based Pruning","metadata":{}},{"cell_type":"code","source":"def prune_with_bandit(model, bandit_agent, pruning_rate, importance_method, dataloader, device=\"cuda\"):\n \n    pruning_rate = min(100, max(0, pruning_rate))\n    \n    print(f\"Base pruning rate: {pruning_rate:.1f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Extract Convolution Layer","metadata":{}},{"cell_type":"code","source":"conv_layers = []\n    conv_layer_names = []\n    for name, module in model.features.named_children():\n        if isinstance(module, nn.Conv2d):\n            conv_layers.append(module)\n            conv_layer_names.append(f\"features.{name}\")\n    \n    num_conv_layers = len(conv_layers)\n    print(f\"Number of convolutional layers: {num_conv_layers}\")\n    \n\n    model.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Calculate importance based on the selected method","metadata":{}},{"cell_type":"code","source":" if importance_method in [\"Sensitivity\"]:\n        # These methods need gradients\n        for param in model.parameters():\n            param.requires_grad_(True)\n            \n       \n        if importance_method == \"Sensitivity\":\n            importance = differential_sensitivity_fusion_pruning(model, dataloader, device=device)\n        \n    else:\n        # Methods that don't need gradients\n        with torch.no_grad():\n            if importance_method == \"Weight\":\n                importance = importance_weight(model)\n            else:\n                raise ValueError(f\"Unknown importance method: {importance_method}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Apply Pruning","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n        total_filters = sum(layer.weight.shape[0] for layer in conv_layers)\n        pruned_filters = 0\n        \n        for layer_idx, (layer, name) in enumerate(zip(conv_layers, conv_layer_names)):\n            # Get importance scores for current layer\n            importance_layer = importance.get(name, None)\n            if importance_layer is None:\n                print(f\"Skipping layer {name}: No importance scores found.\")\n                continue\n\n            if isinstance(importance_layer, np.ndarray):\n                importance_layer = torch.tensor(importance_layer, dtype=torch.float32, device=device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ensure Shape","metadata":{}},{"cell_type":"code","source":"assert importance_layer.shape[0] == layer.weight.shape[0], (\n                f\"Shape mismatch for {name}: {importance_layer.shape} vs {layer.weight.shape}\"\n            )\n\n            # Get layer-specific pruning rate from bandit agent (in percentage)\n            layer_pruning_rate_pct = bandit_agent.select_action(layer_idx, pruning_rate)\n            \n            # Ensure pruning rate is within valid range (10-95%)\n            layer_pruning_rate_pct = max(10, min(95, layer_pruning_rate_pct))\n            \n            # Convert to decimal for internal calculations\n            layer_pruning_rate = layer_pruning_rate_pct / 100.0\n            \n            print(f\"Layer {name}: Pruning {layer_pruning_rate_pct:.1f}% of filters (base rate: {pruning_rate:.1f}%)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Calculate filters to prune","metadata":{}},{"cell_type":"code","source":"num_filters = layer.weight.shape[0]\n            num_filters_to_prune = int(num_filters * layer_pruning_rate)\n            num_filters_to_keep = num_filters - num_filters_to_prune\n\n            # Sort importance scores and determine threshold\n            sorted_importance, sorted_indices = torch.sort(importance_layer, descending=True)\n            threshold = sorted_importance[num_filters_to_keep - 1] if num_filters_to_keep > 0 else float('inf')\n            mask = (importance_layer >= threshold).float().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Count non-zero parameters","metadata":{}},{"cell_type":"code","source":"non_zero_before = layer.weight.data.nonzero().size(0)\n            print(f\"Before pruning {name}: {non_zero_before} non-zero values\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Apply mask to weights and biases","metadata":{}},{"cell_type":"code","source":"layer.weight.data.mul_(mask.view(-1, 1, 1, 1))\n            if layer.bias is not None:\n                layer.bias.data.mul_(mask.view(-1))\n\n non_zero_after = layer.weight.data.nonzero().size(0)\n            print(f\"After pruning {name}: {non_zero_after} non-zero values\")\n            \n           \n            filters_pruned = mask.numel() - mask.sum().item()\n            pruned_filters += filters_pruned\n\n pruning_efficiency = 1.0 - (non_zero_after / non_zero_before)\n            reward = pruning_efficiency * (1.0 - abs(layer_pruning_rate_pct - pruning_rate)/100)\n            \n\n            state = [pruning_rate]  # State is the base pruning rate (percentage)\n            bandit_agent.store_experience(state, layer_pruning_rate_pct, reward)\n\n\n        print(f\"Total filters pruned: {pruned_filters}/{total_filters} ({pruned_filters/total_filters*100:.2f}%)\")\n\n bandit_agent.update_q_values()\n        print(\"Pruning process complete.\")\n        \n    return model\n\nbandit_agent = Banditagent(num_layers=13)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Main execution with the updated pruning methods","metadata":{}},{"cell_type":"code","source":"def generate_pruning_table(model, trainloader, testloader, pruning_rates, device=\"cuda\"):\n    base_model,base_acc= fine_tune_base_model(model, trainloader, testloader, epochs=20, lr=0.001, weight_decay=1e-4, accumulation_steps=4, device=\"cuda\")\n    print(f\"Base model accuracy: {base_acc}\")\n    base_params = count_non_zero_parameters(base_model)\n    base_flops = compute_nonzero_flops(base_model)\n    base_filters = count_nonzero_filters(base_model)\n\n    results = [[\"Base\", round(base_acc, 4), \"-\", base_filters, round(base_params / 1e6, 4), round(base_flops / 1e6, 4)]]\n\n    for rate in pruning_rates:\n        print(f\"Pruning at {rate}%...\")\n        \n        importance_methods = [\"Sensitivity\"]\n\nfor method in importance_methods:\n            print(f\"Applying {method} pruning...\")\n\n            # Load a fresh model copy\n            pruned_model = load_model(device)\n            pruned_model.to(device)\n\n            pruned_model = prune_with_bandit(pruned_model, bandit_agent, rate, method, trainloader, device=\"cuda\")\n\n            acc_pruned = compute_accuracy(pruned_model, testloader, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use knowledge distillation for fine-tuning","metadata":{}},{"cell_type":"code","source":" acc_finetuned = fine_tune_with_kd(\n                student_model=pruned_model,\n                teacher_model=base_model,  # Use the fine-tuned base model as teacher\n                trainloader=trainloader,\n                testloader=testloader,\n                epochs = 1100,\n                device=device\n            )\n            \n            #acc_finetuned = fine_tune_model(pruned_model, trainloader, testloader, epochs=100, lr=0.001, weight_decay=1e-4, accumulation_steps=4, device=\"cuda\")\n            params_pruned = count_non_zero_parameters(pruned_model)\n            flops_pruned = compute_nonzero_flops(pruned_model)\n            filters_pruned = count_nonzero_filters(pruned_model)\n\n            print(\"Accuracy: \", acc_pruned)\n            print(\"Accuracy after finetuning with KD: \", acc_finetuned)\n            print(\"Filters pruned: \", base_filters - filters_pruned)\n            print(\"Flops pruned: \", base_flops - flops_pruned)\n\n            results.append([\n                f\"{method} ({rate}%)\",\n                round(acc_pruned, 4),\n                f\"{round(acc_finetuned, 4)} ({round(100 * acc_finetuned / base_acc, 2)}%)\",\n                filters_pruned,\n                round(params_pruned / 1e6, 4),\n                round(flops_pruned / 1e6, 4)\n            ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Memory Cleanup","metadata":{}},{"cell_type":"code","source":"del pruned_model\n            gc.collect()\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n\n    df = pd.DataFrame(results, columns=[\"Method\", \"Acc. (pruned)\", \"Acc. (fine-tuned)\", \"Filters\", \"Params(M)\", \"FLOPs(M)\"])\n    return df\n\npruning_rates = [50]\n\ntable = generate_pruning_table(model, trainloader, testloader, pruning_rates, device=\"cuda\")\nprint(table)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bandit agent for pruning","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models\nimport numpy as np\nimport random\nimport gc\n\nclass Banditagent:\n    def __init__(self, num_layers, lr=0.01, gamma=0.99, buffer_size=512, mixing_net_hidden=32, epsilon_decay=0.995):\n        self.num_layers = num_layers\n        self.lr = lr\n        self.gamma = gamma\n        self.buffer_size = buffer_size\n        self.epsilon = 1.0  # Initial exploration rate\n        self.epsilon_decay = epsilon_decay\n        # Initialize Q-table with percentage values between 10-90%\n        self.q_table = {layer: random.uniform(10, 90) for layer in range(num_layers)}\n        self.replay_buffer = []\n        self.mixing_net = nn.Sequential(\n            nn.Linear(1, mixing_net_hidden),\n            nn.ReLU(),\n            nn.Linear(mixing_net_hidden, 1)\n        ).to(\"cuda\")\n        self.optimizer = optim.Adam(self.mixing_net.parameters(), lr=lr)\n\n\ndef select_action(self, layer_idx, base_pruning_rate):\n        \"\"\"\n        Select pruning rate for a specific layer based on MIX policy.\n        \n        Args:\n            layer_idx: Index of the layer\n            base_pruning_rate: Base pruning rate (in percentage, e.g., 50, 60, 70)\n            \n        Returns:\n            Pruning rate for the specified layer (in percentage)\n        \"\"\"\n        # Ensure base_pruning_rate is between 0 and 100\n        base_pruning_rate = min(100, max(0, base_pruning_rate))\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Epsilon-greedy policy","metadata":{}},{"cell_type":"code","source":"if random.random() < self.epsilon:\n            # Use a range around the base pruning rate to explore (within ±20%)\n            min_rate = max(10, base_pruning_rate - 5)\n            max_rate = min(90, base_pruning_rate + 5)\n            return random.uniform(min_rate, max_rate)\n        \n        # Use learned Q-value for this layer\n        return self.q_table[layer_idx]\n\ndef store_experience(self, state, action, reward):\n        \"\"\"Store experience in replay buffer\"\"\"\n        if len(self.replay_buffer) >= self.buffer_size:\n            self.replay_buffer.pop(0)\n        self.replay_buffer.append((state, action, reward))\n\n    def update_q_values(self):\n        \"\"\"Update Q-values based on experiences in replay buffer\"\"\"\n        if len(self.replay_buffer) < 32:  # Minimum batch size\n            return\n\nbatch = random.sample(self.replay_buffer, min(len(self.replay_buffer), 64))\n        states, actions, rewards = zip(*batch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"converts to tensors","metadata":{}},{"cell_type":"code","source":"states = torch.tensor(states, dtype=torch.float32).to(\"cuda\")\n        actions = torch.tensor(actions, dtype=torch.float32).view(-1, 1).to(\"cuda\")\n        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1).to(\"cuda\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q-values","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n            target_q_values = self.mixing_net(states) + rewards\n\n        # Compute current Q-values\n        current_q_values = self.mixing_net(states)\n\n        # Compute loss (Mean Squared Error)\n        loss = nn.MSELoss()(current_q_values, target_q_values)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Update","metadata":{}},{"cell_type":"code","source":"self.optimizer.zero_grad()\n        loss.requires_grad = True\n        loss.backward()\n        self.optimizer.step()\n\n        # Decay exploration rate\n        self.epsilon = max(self.epsilon * self.epsilon_decay, 0.01)\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Update Q-vlaue","metadata":{}},{"cell_type":"code","source":" for layer_idx in range(self.num_layers):\n            state_tensor = torch.tensor([[50]], dtype=torch.float32).to(\"cuda\")  # Use 50% as reference state\n            predicted_value = self.mixing_net(state_tensor).item() * 100  # Scale to percentage\n            # Ensure value is within reasonable range (10-90%)\n            self.q_table[layer_idx] = max(10, min(90, predicted_value))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"pruning function","metadata":{}},{"cell_type":"code","source":"def prune_with_bandit(model, bandit_agent, pruning_rate, importance_method, dataloader, device=\"cuda\"):\n \n    # Ensure pruning_rate is within range and in percentage format\n    pruning_rate = min(100, max(0, pruning_rate))\n    \n    print(f\"Base pruning rate: {pruning_rate:.1f}%\")\n    \n    # Extract convolutional layers and their names\n    conv_layers = []\n    conv_layer_names = []\n    for name, module in model.features.named_children():\n        if isinstance(module, nn.Conv2d):\n            conv_layers.append(module)\n            conv_layer_names.append(f\"features.{name}\")\n    \n    num_conv_layers = len(conv_layers)\n    print(f\"Number of convolutional layers: {num_conv_layers}\")\n    \n    # Calculate importance scores\n    model.train()  # Ensure model is in training mode for methods requiring gradients\n    \n    # Calculate importance based on the selected method\n    if importance_method in [\"Sensitivity\"]:\n        # These methods need gradients\n        for param in model.parameters():\n            param.requires_grad_(True)\n            \n       \n        if importance_method == \"Sensitivity\":\n            importance = differential_sensitivity_fusion_pruning(model, dataloader, device=device)\n        \n    else:\n        # Methods that don't need gradients\n        with torch.no_grad():\n            if importance_method == \"Weight\":\n                importance = importance_weight(model)\n            else:\n                raise ValueError(f\"Unknown importance method: {importance_method}\")\n\n    # Apply pruning with no_grad since we don't need gradients for this part\n    with torch.no_grad():\n        total_filters = sum(layer.weight.shape[0] for layer in conv_layers)\n        pruned_filters = 0\n        \n        for layer_idx, (layer, name) in enumerate(zip(conv_layers, conv_layer_names)):\n            # Get importance scores for current layer\n            importance_layer = importance.get(name, None)\n            if importance_layer is None:\n                print(f\"Skipping layer {name}: No importance scores found.\")\n                continue\n\n            if isinstance(importance_layer, np.ndarray):\n                importance_layer = torch.tensor(importance_layer, dtype=torch.float32, device=device)\n\n            # Ensure shape compatibility\n            assert importance_layer.shape[0] == layer.weight.shape[0], (\n                f\"Shape mismatch for {name}: {importance_layer.shape} vs {layer.weight.shape}\"\n            )\n\n            # Get layer-specific pruning rate from bandit agent (in percentage)\n            layer_pruning_rate_pct = bandit_agent.select_action(layer_idx, pruning_rate)\n            \n            # Ensure pruning rate is within valid range (10-95%)\n            layer_pruning_rate_pct = max(10, min(95, layer_pruning_rate_pct))\n            \n            # Convert to decimal for internal calculations\n            layer_pruning_rate = layer_pruning_rate_pct / 100.0\n            \n            print(f\"Layer {name}: Pruning {layer_pruning_rate_pct:.1f}% of filters (base rate: {pruning_rate:.1f}%)\")\n\n            # Calculate how many filters to prune\n            num_filters = layer.weight.shape[0]\n            num_filters_to_prune = int(num_filters * layer_pruning_rate)\n            num_filters_to_keep = num_filters - num_filters_to_prune\n\n            # Sort importance scores and determine threshold\n            sorted_importance, sorted_indices = torch.sort(importance_layer, descending=True)\n            threshold = sorted_importance[num_filters_to_keep - 1] if num_filters_to_keep > 0 else float('inf')\n            mask = (importance_layer >= threshold).float().to(device)\n\n            # Count non-zero parameters before pruning\n            non_zero_before = layer.weight.data.nonzero().size(0)\n            print(f\"Before pruning {name}: {non_zero_before} non-zero values\")\n\n            # Apply mask to weights and biases\n            layer.weight.data.mul_(mask.view(-1, 1, 1, 1))\n            if layer.bias is not None:\n                layer.bias.data.mul_(mask.view(-1))\n\n            # Count non-zero parameters after pruning\n            non_zero_after = layer.weight.data.nonzero().size(0)\n            print(f\"After pruning {name}: {non_zero_after} non-zero values\")\n            \n            # Calculate pruning effectiveness\n            filters_pruned = mask.numel() - mask.sum().item()\n            pruned_filters += filters_pruned\n            \n            # Calculate reward based on pruning effectiveness (using percentages)\n            pruning_efficiency = 1.0 - (non_zero_after / non_zero_before)\n            reward = pruning_efficiency * (1.0 - abs(layer_pruning_rate_pct - pruning_rate)/100)\n            \n            # Store experience\n            state = [pruning_rate]  # State is the base pruning rate (percentage)\n            bandit_agent.store_experience(state, layer_pruning_rate_pct, reward)\n\n        # Final statistics\n        print(f\"Total filters pruned: {pruned_filters}/{total_filters} ({pruned_filters/total_filters*100:.2f}%)\")\n        \n        # Update the agent\n        bandit_agent.update_q_values()\n        print(\"Pruning process complete.\")\n        \n    return model\n\n# Initialize the \"Agent\"\nbandit_agent = Banditagent(num_layers=13)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Updated pruning method","metadata":{}},{"cell_type":"code","source":"def generate_pruning_table(model, trainloader, testloader, pruning_rates, device=\"cuda\"):\n    base_model,base_acc= fine_tune_base_model(model, trainloader, testloader, epochs=20, lr=0.001, weight_decay=1e-4, accumulation_steps=4, device=\"cuda\")\n    print(f\"Base model accuracy: {base_acc}\")\n    base_params = count_non_zero_parameters(base_model)\n    base_flops = compute_nonzero_flops(base_model)\n    base_filters = count_nonzero_filters(base_model)\n\n    results = [[\"Base\", round(base_acc, 4), \"-\", base_filters, round(base_params / 1e6, 4), round(base_flops / 1e6, 4)]]\n\n    for rate in pruning_rates:\n        print(f\"Pruning at {rate}%...\")\n        \n        importance_methods = [\"Sensitivity\"]\n\n        for method in importance_methods:\n            print(f\"Applying {method} pruning...\")\n\n            # Load a fresh model copy\n            pruned_model = load_model(device)\n            pruned_model.to(device)\n\n            pruned_model = prune_with_bandit(pruned_model, bandit_agent, rate, method, trainloader, device=\"cuda\")\n\n            acc_pruned = compute_accuracy(pruned_model, testloader, device)\n\n            # Use knowledge distillation for fine-tuning\n           \n            acc_finetuned = fine_tune_with_kd(\n                student_model=pruned_model,\n                teacher_model=base_model,  # Use the fine-tuned base model as teacher\n                trainloader=trainloader,\n                testloader=testloader,\n                epochs = 1000,\n                device=device\n            )\n            \n            #acc_finetuned = fine_tune_model(pruned_model, trainloader, testloader, epochs=100, lr=0.001, weight_decay=1e-4, accumulation_steps=4, device=\"cuda\")\n            params_pruned = count_non_zero_parameters(pruned_model)\n            flops_pruned = compute_nonzero_flops(pruned_model)\n            filters_pruned = count_nonzero_filters(pruned_model)\n\n            print(\"Accuracy: \", acc_pruned)\n            print(\"Accuracy after finetuning with KD: \", acc_finetuned)\n            print(\"Filters pruned: \", base_filters - filters_pruned)\n            print(\"Flops pruned: \", base_flops - flops_pruned)\n\n            results.append([\n                f\"{method} ({rate}%)\",\n                round(acc_pruned, 4),\n                f\"{round(acc_finetuned, 4)} ({round(100 * acc_finetuned / base_acc, 2)}%)\",\n                filters_pruned,\n                round(params_pruned / 1e6, 4),\n                round(flops_pruned / 1e6, 4)\n            ])\n\n            # Memory Cleanup\n            del pruned_model\n            gc.collect()\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n\n    df = pd.DataFrame(results, columns=[\"Method\", \"Acc. (pruned)\", \"Acc. (fine-tuned)\", \"Filters\", \"Params(M)\", \"FLOPs(M)\"])\n    return df\n\n\npruning_rates = [60]\n\ntable = generate_pruning_table(model, trainloader, testloader, pruning_rates, device=\"cuda\")\nprint(table)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bandit Agent for Pruning","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models\nimport numpy as np\nimport random\nimport gc\n\nclass Banditagent:\n    def __init__(self, num_layers, lr=0.01, gamma=0.99, buffer_size=512, mixing_net_hidden=32, epsilon_decay=0.995):\n        self.num_layers = num_layers\n        self.lr = lr\n        self.gamma = gamma\n        self.buffer_size = buffer_size\n        self.epsilon = 1.0  # Initial exploration rate\n        self.epsilon_decay = epsilon_decay\n        # Initialize Q-table with percentage values between 10-90%\n        self.q_table = {layer: random.uniform(10, 90) for layer in range(num_layers)}\n        self.replay_buffer = []\n        self.mixing_net = nn.Sequential(\n            nn.Linear(1, mixing_net_hidden),\n            nn.ReLU(),\n            nn.Linear(mixing_net_hidden, 1)\n        ).to(\"cuda\")\n        self.optimizer = optim.Adam(self.mixing_net.parameters(), lr=lr)\n        \n    def select_action(self, layer_idx, base_pruning_rate):\n        \"\"\"\n        Select pruning rate for a specific layer based on MIX policy.\n        \n        Args:\n            layer_idx: Index of the layer\n            base_pruning_rate: Base pruning rate (in percentage, e.g., 50, 60, 70)\n            \n        Returns:\n            Pruning rate for the specified layer (in percentage)\n        \"\"\"\n        # Ensure base_pruning_rate is between 0 and 100\n        base_pruning_rate = min(100, max(0, base_pruning_rate))\n        \n        # Epsilon-greedy policy\n        if random.random() < self.epsilon:\n            # Use a range around the base pruning rate to explore (within ±20%)\n            min_rate = max(10, base_pruning_rate - 5)\n            max_rate = min(90, base_pruning_rate + 5)\n            return random.uniform(min_rate, max_rate)\n\n return self.q_table[layer_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Update Q-value","metadata":{}},{"cell_type":"code","source":"def store_experience(self, state, action, reward):\n        \"\"\"Store experience in replay buffer\"\"\"\n        if len(self.replay_buffer) >= self.buffer_size:\n            self.replay_buffer.pop(0)\n        self.replay_buffer.append((state, action, reward))\n\n    def update_q_values(self):\n        \"\"\"Update Q-values based on experiences in replay buffer\"\"\"\n        if len(self.replay_buffer) < 32:  # Minimum batch size\n            return  # Not enough experiences to update\n            \n        # Sample a batch of experiences\n        batch = random.sample(self.replay_buffer, min(len(self.replay_buffer), 64))\n        states, actions, rewards = zip(*batch)\n\nstates = torch.tensor(states, dtype=torch.float32).to(\"cuda\")\n        actions = torch.tensor(actions, dtype=torch.float32).view(-1, 1).to(\"cuda\")\n        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1).to(\"cuda\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q-value","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n            target_q_values = self.mixing_net(states) + rewards\n\n        # Compute current Q-values\n        current_q_values = self.mixing_net(states)\n\n        # Compute loss (Mean Squared Error)\n        loss = nn.MSELoss()(current_q_values, target_q_values)\n\n        # Update parameters\n        self.optimizer.zero_grad()\n        loss.requires_grad = True\n        loss.backward()\n        self.optimizer.step()\n\nself.epsilon = max(self.epsilon * self.epsilon_decay, 0.01)\n        \n        # Update q_table values for future action selection\n        for layer_idx in range(self.num_layers):\n            state_tensor = torch.tensor([[50]], dtype=torch.float32).to(\"cuda\")  # Use 50% as reference state\n            predicted_value = self.mixing_net(state_tensor).item() * 100  # Scale to percentage\n            # Ensure value is within reasonable range (10-90%)\n            self.q_table[layer_idx] = max(10, min(90, predicted_value))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bandit-based Pruning function","metadata":{}},{"cell_type":"code","source":"def prune_with_bandit(model, bandit_agent, pruning_rate, importance_method, dataloader, device=\"cuda\"):\n \n    # Ensure pruning_rate is within range and in percentage format\n    pruning_rate = min(100, max(0, pruning_rate))\n    \n    print(f\"Base pruning rate: {pruning_rate:.1f}%\")\n    \n    # Extract convolutional layers and their names\n    conv_layers = []\n    conv_layer_names = []\n    for name, module in model.features.named_children():\n        if isinstance(module, nn.Conv2d):\n            conv_layers.append(module)\n            conv_layer_names.append(f\"features.{name}\")\n    \n    num_conv_layers = len(conv_layers)\n    print(f\"Number of convolutional layers: {num_conv_layers}\")\n    \n    # Calculate importance scores\n    model.train()  # Ensure model is in training mode for methods requiring gradients\n    \n    # Calculate importance based on the selected method\n    if importance_method in [\"Sensitivity\"]:\n        # These methods need gradients\n        for param in model.parameters():\n            param.requires_grad_(True)\n            \n       \n        if importance_method == \"Sensitivity\":\n            importance = differential_sensitivity_fusion_pruning(model, dataloader, device=device)\n        \n    else:\n        # Methods that don't need gradients\n        with torch.no_grad():\n            if importance_method == \"Weight\":\n                importance = importance_weight(model)\n            else:\n                raise ValueError(f\"Unknown importance method: {importance_method}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Apply pruning with no-grad ","metadata":{}},{"cell_type":"code","source":" with torch.no_grad():\n        total_filters = sum(layer.weight.shape[0] for layer in conv_layers)\n        pruned_filters = 0\n        \n        for layer_idx, (layer, name) in enumerate(zip(conv_layers, conv_layer_names)):\n            # Get importance scores for current layer\n            importance_layer = importance.get(name, None)\n            if importance_layer is None:\n                print(f\"Skipping layer {name}: No importance scores found.\")\n                continue\n\n            if isinstance(importance_layer, np.ndarray):\n                importance_layer = torch.tensor(importance_layer, dtype=torch.float32, device=device)\n\n            # Ensure shape compatibility\n            assert importance_layer.shape[0] == layer.weight.shape[0], (\n                f\"Shape mismatch for {name}: {importance_layer.shape} vs {layer.weight.shape}\"\n            )\n\n            # Get layer-specific pruning rate from bandit agent (in percentage)\n            layer_pruning_rate_pct = bandit_agent.select_action(layer_idx, pruning_rate)\n            \n            # Ensure pruning rate is within valid range (10-95%)\n            layer_pruning_rate_pct = max(10, min(95, layer_pruning_rate_pct))\n            \n            # Convert to decimal for internal calculations\n            layer_pruning_rate = layer_pruning_rate_pct / 100.0\n            \n            print(f\"Layer {name}: Pruning {layer_pruning_rate_pct:.1f}% of filters (base rate: {pruning_rate:.1f}%)\")\n\n            # Calculate how many filters to prune\n            num_filters = layer.weight.shape[0]\n            num_filters_to_prune = int(num_filters * layer_pruning_rate)\n            num_filters_to_keep = num_filters - num_filters_to_prune\n\n\n sorted_importance, sorted_indices = torch.sort(importance_layer, descending=True)\n            threshold = sorted_importance[num_filters_to_keep - 1] if num_filters_to_keep > 0 else float('inf')\n            mask = (importance_layer >= threshold).float().to(device)\n\n            # Count non-zero parameters before pruning\n            non_zero_before = layer.weight.data.nonzero().size(0)\n            print(f\"Before pruning {name}: {non_zero_before} non-zero values\")\n\n            # Apply mask to weights and biases\n            layer.weight.data.mul_(mask.view(-1, 1, 1, 1))\n            if layer.bias is not None:\n                layer.bias.data.mul_(mask.view(-1))\n\n            # Count non-zero parameters after pruning\n            non_zero_after = layer.weight.data.nonzero().size(0)\n            print(f\"After pruning {name}: {non_zero_after} non-zero values\")\n            \n            # Calculate pruning effectiveness\n            filters_pruned = mask.numel() - mask.sum().item()\n            pruned_filters += filters_pruned\n            \n            # Calculate reward based on pruning effectiveness (using percentages)\n            pruning_efficiency = 1.0 - (non_zero_after / non_zero_before)\n            reward = pruning_efficiency * (1.0 - abs(layer_pruning_rate_pct - pruning_rate)/100)\n            \n            # Store experience\n            state = [pruning_rate]  # State is the base pruning rate (percentage)\n            bandit_agent.store_experience(state, layer_pruning_rate_pct, reward)\n\n        # Final statistics\n        print(f\"Total filters pruned: {pruned_filters}/{total_filters} ({pruned_filters/total_filters*100:.2f}%)\")\n        \n        # Update the agent\n        bandit_agent.update_q_values()\n        print(\"Pruning process complete.\")\n        \n    return model\n\n# Initialize the \"Agent\"\nbandit_agent = Banditagent(num_layers=13)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Main execution with the updated pruning methods","metadata":{}},{"cell_type":"code","source":"def generate_pruning_table(model, trainloader, testloader, pruning_rates, device=\"cuda\"):\n    base_model,base_acc= fine_tune_base_model(model, trainloader, testloader, epochs=20, lr=0.001, weight_decay=1e-4, accumulation_steps=4, device=\"cuda\")\n    print(f\"Base model accuracy: {base_acc}\")\n    base_params = count_non_zero_parameters(base_model)\n    base_flops = compute_nonzero_flops(base_model)\n    base_filters = count_nonzero_filters(base_model)\n\n    results = [[\"Base\", round(base_acc, 4), \"-\", base_filters, round(base_params / 1e6, 4), round(base_flops / 1e6, 4)]]\n\n    for rate in pruning_rates:\n        print(f\"Pruning at {rate}%...\")\n        \n        importance_methods = [\"Sensitivity\"]\n\n        for method in importance_methods:\n            print(f\"Applying {method} pruning...\")\n\n            # Load a fresh model copy\n            pruned_model = load_model(device)\n            pruned_model.to(device)\n\n            pruned_model = prune_with_bandit(pruned_model, bandit_agent, rate, method, trainloader, device=\"cuda\")\n\n            acc_pruned = compute_accuracy(pruned_model, testloader, device)\n\n            # Use knowledge distillation for fine-tuning\n           \n            acc_finetuned = fine_tune_with_kd(\n                student_model=pruned_model,\n                teacher_model=base_model,  # Use the fine-tuned base model as teacher\n                trainloader=trainloader,\n                testloader=testloader,\n                epochs = 800,\n                device=device\n            )\n            \n            #acc_finetuned = fine_tune_model(pruned_model, trainloader, testloader, epochs=100, lr=0.001, weight_decay=1e-4, accumulation_steps=4, device=\"cuda\")\n            params_pruned = count_non_zero_parameters(pruned_model)\n            flops_pruned = compute_nonzero_flops(pruned_model)\n            filters_pruned = count_nonzero_filters(pruned_model)\n\n            print(\"Accuracy: \", acc_pruned)\n            print(\"Accuracy after finetuning with KD: \", acc_finetuned)\n            print(\"Filters pruned: \", base_filters - filters_pruned)\n            print(\"Flops pruned: \", base_flops - flops_pruned)\n\n            results.append([\n                f\"{method} ({rate}%)\",\n                round(acc_pruned, 4),\n                f\"{round(acc_finetuned, 4)} ({round(100 * acc_finetuned / base_acc, 2)}%)\",\n                filters_pruned,\n                round(params_pruned / 1e6, 4),\n                round(flops_pruned / 1e6, 4)\n            ])\n\n            # Memory Cleanup\n            del pruned_model\n            gc.collect()\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n\n    df = pd.DataFrame(results, columns=[\"Method\", \"Acc. (pruned)\", \"Acc. (fine-tuned)\", \"Filters\", \"Params(M)\", \"FLOPs(M)\"])\n    return df\n\n\npruning_rates = [70]\n\ntable = generate_pruning_table(model, trainloader, testloader, pruning_rates, device=\"cuda\")\nprint(table)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}